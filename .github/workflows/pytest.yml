name: Test Package

on: [ push, pull_request ]

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  GH_TOKEN: ${{ github.token }}
  UV_SYSTEM_PYTHON: 1

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [ "3.10", "3.11", "3.12", "3.13" ]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Install dependencies
        run: uv pip install -Ur requirements.txt

      # build vllm
      - name: Find latest vLLM release
        id: vllm_release
        run: |
          echo "tag=$(gh release list --repo vllm-project/vllm --json name,isLatest,tagName --jq '.[] | select(.isLatest) | .tagName')" >> "$GITHUB_OUTPUT"

      - name: Checkout vLLM
        uses: actions/checkout@v4
        with:
          repository: vllm-project/vllm
          path: _vllm
          ref: ${{ steps.vllm_release.outputs.tag }}

      - name: Download vLLM CPU deps
        working-directory: _vllm
        run: |
          sudo apt-get update -y
          sudo apt-get install -y gcc-12 g++-12 libnuma-dev python3-dev
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
          uv pip install -r requirements/cpu-build.txt --torch-backend cpu
          uv pip install -r requirements/cpu.txt --torch-backend cpu

      - name: Build vLLM CPU
        working-directory: _vllm
        run: VLLM_TARGET_DEVICE=cpu uv pip install . --no-build-isolation

      # download models
      - name: Cache HF models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-models

      - name: Download HF models
        run: |
          huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --exclude "original/*"

      - name: Test with pytest
        run: pytest --ignore=_vllm -rA

