name: Test Package

on: [ push, pull_request ]

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  GH_TOKEN: ${{ github.token }}

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [ "3.10", "3.11", "3.12" ]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -U kani pytest pytest-asyncio

      # build vllm
      - name: Find latest vLLM release
        id: vllm_release
        run: |
          echo "tag=$(gh release list --repo vllm-project/vllm --json name,isLatest,tagName --jq '.[] | select(.isLatest) | .tagName')" >> "$GITHUB_OUTPUT"

      - name: Checkout vLLM
        uses: actions/checkout@v4
        with:
          repository: vllm-project/vllm
          path: _vllm
          ref: ${{ steps.vllm_release.outputs.tag }}

      - name: Download vLLM CPU deps
        working-directory: _vllm
        run: |
          sudo apt-get update  -y
          sudo apt-get install -y gcc-12 g++-12 libnuma-dev
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
          pip install --upgrade pip
          pip install cmake>=3.26 wheel packaging ninja "setuptools-scm>=8" numpy
          pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu

      - name: Build vLLM CPU
        working-directory: _vllm
        run: VLLM_TARGET_DEVICE=cpu python setup.py install

      # download models
      - name: Cache HF models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-models

      - name: Download HF models
        run: |
          huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --exclude "original/*"

      - name: Test with pytest
        run: pytest --ignore=_vllm

