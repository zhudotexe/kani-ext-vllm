"""
Ensure that the tokens generated by the offline vLLM engine are the same as the tokens in the API mode.

We do this by just spinning up one of each and seeing what it outputs. This runs on CPU, so we use a tiny model.
"""

import itertools
import random

import pytest
from pytest_lazy_fixtures import lf

from tests.conftest import PROMPTS, infer_with_engine


# pairwise equivalence tests
@pytest.mark.parametrize(
    ["e1", "e2"],
    itertools.pairwise([lf("offline_engine"), lf("api_engine"), lf("openai_engine")]),
)
async def test_equivalence(e1, e2):
    prompt = random.choice(PROMPTS)
    resp1 = await infer_with_engine(e1, prompt)
    print(f"{type(e1).__name__}: {resp1}")
    resp2 = await infer_with_engine(e2, prompt)
    print(f"{type(e2).__name__}: {resp2}")
    assert resp1 == resp2


@pytest.mark.parametrize("engine", [lf("offline_engine"), lf("api_engine"), lf("openai_engine")])
async def test_equivalence_stream(engine):
    prompt = random.choice(PROMPTS)
    resp1 = await infer_with_engine(engine, prompt, stream=False)
    print(f"{type(engine).__name__} (no stream): {resp1}")
    resp2 = await infer_with_engine(engine, prompt, stream=True)
    print(f"{type(engine).__name__} (streaming): {resp2}")
    assert resp1 == resp2
